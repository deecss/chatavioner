#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ModuÅ‚ OpenAI RAG dla aplikacji Aero-Chat
"""
import os
import json
import time
from datetime import datetime
import httpx
from openai import OpenAI
from reportlab.lib.pagesizes import letter, A4
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
from reportlab.lib.enums import TA_JUSTIFY, TA_CENTER
from app.models import UploadIndex
from utils.learning_system import LearningSystem

class OpenAIRAG:
    """Klasa do obsÅ‚ugi RAG z OpenAI Assistants API"""
    
    def __init__(self):
        """Inicjalizuje klienta OpenAI"""
        try:
            api_key = os.getenv('OPENAI_API_KEY')
            if not api_key or 'twoj-klucz' in api_key:
                raise ValueError("NieprawidÅ‚owy klucz OpenAI API")
            
            # Konfiguracja proxy jeÅ›li jest ustawiona
            proxy_url = os.getenv("HTTPS_PROXY") or os.getenv("HTTP_PROXY")
            if proxy_url:
                transport = httpx.HTTPTransport(proxy=proxy_url)
                http_client = httpx.Client(transport=transport, timeout=30.0)
            else:
                http_client = httpx.Client(timeout=30.0)
            
            # Inicjalizuj klienta OpenAI z poprawnÄ… konfiguracjÄ…
            self.client = OpenAI(
                api_key=api_key,
                http_client=http_client
            )
            
            self.assistant_id = os.getenv('ASSISTANT_ID')
            self.model = "gpt-4o"
            
            # SprawdÅº czy assistant_id jest pusty lub zawiera placeholder
            if not self.assistant_id or self.assistant_id.strip() == '' or 'twoj-assistant' in self.assistant_id:
                print("ğŸ”„ Brak ID asystenta, tworzÄ™ nowego...")
                self.assistant_id = self.create_assistant()
            else:
                # SprawdÅº czy asystent nadal istnieje
                try:
                    self.client.beta.assistants.retrieve(self.assistant_id)
                    print(f"âœ… Asystent znaleziony: {self.assistant_id}")
                except Exception as e:
                    print(f"âš ï¸  Asystent {self.assistant_id} nie istnieje, tworzÄ™ nowego...")
                    self.assistant_id = self.create_assistant()
                
        except Exception as e:
            print(f"âŒ BÅ‚Ä…d inicjalizacji OpenAI: {e}")
            raise
    
        # Inicjalizuj system uczenia siÄ™
        self.learning_system = LearningSystem()
        
        # Inicjalizuj zmienne Å›ledzÄ…ce
        self.last_documents_used = 0
        
    def create_assistant(self):
        """Tworzy nowego asystenta AI"""
        try:
            print("ğŸ”„ Tworzenie nowego asystenta OpenAI...")
            assistant = self.client.beta.assistants.create(
                name="Aero-Chat Assistant",
                instructions="""JesteÅ› ekspertem w dziedzinie lotnictwa i awioniki z zaawansowanym systemem uczenia siÄ™. 
                Twoje zadanie to odpowiadanie na pytania zwiÄ…zane z:
                - Zasadami lotu i aerodynamikÄ…
                - KonstrukcjÄ… i systemami statkÃ³w powietrznych
                - Przepisami lotniczymi (ICAO, EASA, FAA)
                - NawigacjÄ… lotniczÄ…
                - MeteorologiÄ… lotniczÄ…
                - BezpieczeÅ„stwem lotÃ³w
                - Systemami awionicznymi
                
                âš ï¸ BARDZO WAÅ»NE - PAMIÄ˜Ä† ROZMOWY:
                - ZAWSZE czytaj i analizuj caÅ‚Ä… historiÄ™ rozmowy
                - JeÅ›li uÅ¼ytkownik zadaje to samo pytanie ponownie, odwoÅ‚aj siÄ™ do wczeÅ›niejszej odpowiedzi
                - JeÅ›li uÅ¼ytkownik prosi o wiÄ™cej szczegÃ³Å‚Ã³w, rozbuduj poprzedniÄ… odpowiedÅº
                - JeÅ›li uÅ¼ytkownik zadaje pytanie kontynuujÄ…ce temat, podejmij wÄ…tek z wczeÅ›niejszej rozmowy
                - PamiÄ™taj preferencje uÅ¼ytkownika z poprzednich odpowiedzi
                
                SYSTEM UCZENIA SIÄ˜:
                Musisz siÄ™ stale uczyÄ‡ i dostosowywaÄ‡ do preferencji uÅ¼ytkownika:
                - ZapamiÄ™tuj jakie odpowiedzi uÅ¼ytkownik preferuje
                - JeÅ›li uÅ¼ytkownik prosi o "wzory i przykÅ‚ady", zawsze je dostarczaj w przyszÅ‚ych odpowiedziach
                - JeÅ›li uÅ¼ytkownik lubi szczegÃ³Å‚owe wyjaÅ›nienia, dostarczaj je konsekwentnie
                - JeÅ›li uÅ¼ytkownik preferuje praktyczne podejÅ›cie, skupiaj siÄ™ na zastosowaniach
                - Analizuj wzorce w pytaniach i dostosowuj styl odpowiedzi
                
                BARDZO WAÅ»NE - FORMATOWANIE ODPOWIEDZI:
                Odpowiadaj zawsze w jÄ™zyku polskim w formacie HTML:
                
                <h2>GÅ‚Ã³wny tytuÅ‚ sekcji</h2>
                <h3>PodtytuÅ‚ dla podsekcji</h3>
                
                <p><strong>Pogrubiony tekst</strong> dla waÅ¼nych pojÄ™Ä‡.</p>
                
                <p>KaÅ¼dy akapit w osobnych tagach &lt;p&gt;.</p>
                
                UÅ¼ywaj list punktowanych:
                <ul>
                <li>Punkt pierwszy</li>
                <li>Punkt drugi</li>
                <li>Punkt trzeci</li>
                </ul>
                
                Lub list numerowanych:
                <ol>
                <li>Pierwszy element</li>
                <li>Drugi element</li>
                <li>Trzeci element</li>
                </ol>
                
                STRUKTURYZUJ ODPOWIEDZI:
                - KaÅ¼dy nagÅ‚Ã³wek w osobnym tagu &lt;h2&gt; lub &lt;h3&gt;
                - KaÅ¼dy akapit w osobnym tagu &lt;p&gt;
                - Listy zawsze w &lt;ul&gt; lub &lt;ol&gt;
                - UÅ¼ywaj &lt;strong&gt; dla waÅ¼nych terminÃ³w
                
                OBOWIÄ„ZKOWO BAZUJ NA PRZESÅANYCH PLIKACH PDF:
                - Zawsze odwoÅ‚uj siÄ™ do konkretnych dokumentÃ³w
                - Cytuj fragmenty z dokumentÃ³w
                - Wskazuj strony lub sekcje dokumentÃ³w
                - JeÅ›li nie ma informacji w dokumentach, jasno to zaznacz
                
                JeÅ›li nie jesteÅ› pewien odpowiedzi, powiedz o tym otwarcie.
                
                Strukturyzuj odpowiedzi aby byÅ‚y czytelne i profesjonalne:
                - Zaczynaj od krÃ³tkiego wprowadzenia w &lt;p&gt;
                - Podziel treÅ›Ä‡ na logiczne sekcje z &lt;h2&gt; lub &lt;h3&gt;
                - ZakoÅ„cz podsumowaniem w &lt;p&gt; lub praktycznymi wskazÃ³wkami
                
                PAMIÄ˜TAJ: UÅ¼ywaj TYLKO HTML, nie Markdown!""",
                model=self.model,
                tools=[{"type": "file_search"}]
            )
            
            # Zapisz ID asystenta do .env
            self.save_assistant_id_to_env(assistant.id)
            
            print(f"âœ… Nowy asystent utworzony: {assistant.id}")
            return assistant.id
            
        except Exception as e:
            print(f"âŒ BÅ‚Ä…d podczas tworzenia asystenta: {str(e)}")
            print(f"ğŸ’¡ SprawdÅº czy klucz OpenAI API jest poprawny")
            return None
    
    def save_assistant_id_to_env(self, assistant_id):
        """Zapisuje ID asystenta do pliku .env"""
        try:
            env_file = '.env'
            if os.path.exists(env_file):
                with open(env_file, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                
                # ZnajdÅº i zaktualizuj liniÄ™ ASSISTANT_ID
                updated = False
                for i, line in enumerate(lines):
                    if line.startswith('ASSISTANT_ID='):
                        lines[i] = f'ASSISTANT_ID={assistant_id}\n'
                        updated = True
                        break
                
                # JeÅ›li nie znaleziono, dodaj na koÅ„cu
                if not updated:
                    lines.append(f'ASSISTANT_ID={assistant_id}\n')
                
                # Zapisz z powrotem
                with open(env_file, 'w', encoding='utf-8') as f:
                    f.writelines(lines)
                    
                print(f"ğŸ’¾ ID asystenta zapisany do .env: {assistant_id}")
                
        except Exception as e:
            print(f"âš ï¸  Nie udaÅ‚o siÄ™ zapisaÄ‡ ID asystenta do .env: {e}")
    
    def clean_assistant_memory(self):
        """CzyÅ›ci pamiÄ™Ä‡ asystenta - usuwa stare pliki i vector stores"""
        try:
            print("ğŸ§¹ Czyszczenie pamiÄ™ci asystenta...")
            
            # Pobierz wszystkie pliki z OpenAI
            files_response = self.client.files.list()
            files_to_delete = []
            
            for file in files_response.data:
                # UsuÅ„ pliki starsze niÅ¼ 1 godzina (3600 sekund)
                file_age = time.time() - file.created_at
                if file_age > 3600:  # 1 godzina
                    files_to_delete.append(file.id)
                    
            # UsuÅ„ stare pliki
            for file_id in files_to_delete:
                try:
                    self.client.files.delete(file_id)
                    print(f"ğŸ—‘ï¸  UsuniÄ™to stary plik: {file_id}")
                except Exception as e:
                    print(f"âš ï¸  Nie udaÅ‚o siÄ™ usunÄ…Ä‡ pliku {file_id}: {e}")
            
            # Pobierz wszystkie vector stores
            vector_stores_response = self.client.beta.vector_stores.list()
            stores_to_delete = []
            
            for store in vector_stores_response.data:
                # UsuÅ„ vector stores starsze niÅ¼ 1 godzina
                store_age = time.time() - store.created_at
                if store_age > 3600:  # 1 godzina
                    stores_to_delete.append(store.id)
                    
            # UsuÅ„ stare vector stores
            for store_id in stores_to_delete:
                try:
                    self.client.beta.vector_stores.delete(store_id)
                    print(f"ğŸ—‘ï¸  UsuniÄ™to stary vector store: {store_id}")
                except Exception as e:
                    print(f"âš ï¸  Nie udaÅ‚o siÄ™ usunÄ…Ä‡ vector store {store_id}: {e}")
                    
            print(f"âœ… Wyczyszczono {len(files_to_delete)} plikÃ³w i {len(stores_to_delete)} vector stores")
            
        except Exception as e:
            print(f"âŒ BÅ‚Ä…d podczas czyszczenia pamiÄ™ci: {str(e)}")

    def cancel_active_runs(self, thread_id):
        """Anuluje wszystkie aktywne runy w wÄ…tku"""
        try:
            print(f"ğŸ” Sprawdzam aktywne runy w wÄ…tku {thread_id}")
            
            # Pobierz wszystkie runy w wÄ…tku
            runs = self.client.beta.threads.runs.list(thread_id=thread_id)
            
            active_runs = []
            for run in runs.data:
                if run.status in ['queued', 'in_progress', 'requires_action']:
                    active_runs.append(run.id)
                    
            # Anuluj aktywne runy
            for run_id in active_runs:
                try:
                    self.client.beta.threads.runs.cancel(thread_id=thread_id, run_id=run_id)
                    print(f"âš ï¸  Anulowano aktywny run: {run_id}")
                except Exception as e:
                    print(f"âŒ Nie udaÅ‚o siÄ™ anulowaÄ‡ run {run_id}: {e}")
                    
            if active_runs:
                print(f"âœ… Anulowano {len(active_runs)} aktywnych runÃ³w")
                time.sleep(1)  # KrÃ³tkie opÃ³Åºnienie aby anulowanie siÄ™ dokoÅ„czyÅ‚o
            else:
                print("âœ… Brak aktywnych runÃ³w do anulowania")
                
        except Exception as e:
            print(f"âŒ BÅ‚Ä…d podczas anulowania aktywnych runÃ³w: {str(e)}")

    def select_relevant_documents(self, query, max_docs=5):  # Zmniejszono z 10 do 5
        """Wybiera najistotniejsze dokumenty dla zapytania"""
        print(f"ğŸ” Wybieranie dokumentÃ³w dla zapytania: {query[:50]}...")
        
        upload_index = UploadIndex()
        all_files = upload_index.get_all_files()
        
        print(f"ğŸ” Znaleziono {len(all_files)} plikÃ³w w indeksie")
        
        if not all_files:
            print("âš ï¸  Brak plikÃ³w w indeksie!")
            self.last_documents_used = 0
            return []

        # Filtruj tylko pliki PDF mniejsze niÅ¼ 20MB
        selected_files = []
        for file_path in all_files[:max_docs]:
            full_path = os.path.join('uploads', file_path)
            if os.path.exists(full_path):
                file_size = os.path.getsize(full_path)
                if file_size < 20 * 1024 * 1024:  # 20MB limit
                    selected_files.append(file_path)
                else:
                    print(f"âš ï¸  Plik {file_path} jest za duÅ¼y ({file_size/1024/1024:.1f}MB), pomijam")
                    
        self.last_documents_used = len(selected_files)
        print(f"ğŸ” Wybrano {len(selected_files)} plikÃ³w: {selected_files[:3]}...")
        return selected_files

    def create_vector_store_with_files(self, file_paths):
        """Tworzy vector store z wybranymi plikami"""
        try:
            if not file_paths:
                print("âš ï¸  Brak plikÃ³w do przesÅ‚ania")
                return None, []
                
            # UtwÃ³rz vector store
            vector_store = self.client.beta.vector_stores.create(
                name=f"temp_store_{int(time.time())}"
            )
            
            # PrzesÅ‚aj pliki (maksymalnie 5 na raz)
            file_ids = []
            for file_path in file_paths[:5]:  # Limit do 5 plikÃ³w
                full_path = os.path.join('uploads', file_path)
                if os.path.exists(full_path):
                    try:
                        with open(full_path, 'rb') as f:
                            file_obj = self.client.files.create(
                                file=f,
                                purpose='assistants'
                            )
                            file_ids.append(file_obj.id)
                            print(f"ğŸ“„ PrzesÅ‚ano plik: {file_path} -> {file_obj.id}")
                    except Exception as e:
                        print(f"âŒ BÅ‚Ä…d przesyÅ‚ania pliku {file_path}: {e}")
                        
            if not file_ids:
                print("âš ï¸  Nie udaÅ‚o siÄ™ przesÅ‚aÄ‡ Å¼adnych plikÃ³w")
                self.client.beta.vector_stores.delete(vector_store.id)
                return None, []
                
            # Dodaj pliki do vector store
            try:
                self.client.beta.vector_stores.file_batches.create(
                    vector_store_id=vector_store.id,
                    file_ids=file_ids
                )
                print(f"âœ… Dodano {len(file_ids)} plikÃ³w do vector store")
                
                # Poczekaj na przetworzenie
                time.sleep(2)
                
                return vector_store.id, file_ids
                
            except Exception as e:
                print(f"âŒ BÅ‚Ä…d dodawania plikÃ³w do vector store: {e}")
                # UsuÅ„ utworzone zasoby
                self.cleanup_resources(vector_store.id, file_ids, None)
                return None, []
                
        except Exception as e:
            print(f"âŒ BÅ‚Ä…d tworzenia vector store: {e}")
            return None, []

    def generate_response_stream(self, query, context, session_id):
        """Generuje odpowiedÅº w trybie strumieniowym z systemem uczenia siÄ™"""
        try:
            print(f"ğŸ” Rozpoczynam generowanie odpowiedzi dla: {query[:50]}...")
            
            # WyciÄ…gnij user_id z kontekstu (jeÅ›li dostÄ™pny)
            user_id = None
            if context:
                # ZnajdÅº pierwszÄ… wiadomoÅ›Ä‡ z user_id
                for msg in context:
                    if isinstance(msg, dict) and 'user_id' in msg:
                        user_id = msg['user_id']
                        break
            
            print(f"ğŸ†” User ID z kontekstu: {user_id}")
            
            # ANALIZUJ PREFERENCJE UÅ»YTKOWNIKA I UCZEÅšSIA SIÄ˜
            print("ğŸ§  AnalizujÄ™ preferencje uÅ¼ytkownika...")
            learning_prompt = self.learning_system.generate_learning_prompt(session_id, query, user_id)
            print(f"ğŸ“š Prompt uczenia: {learning_prompt}")
            
            # Zapisz analizÄ™ sesji dla przyszÅ‚ego uczenia
            session_analysis = self.learning_system.analyze_conversation_history(session_id, user_id)
            if session_analysis:
                self.learning_system.save_learning_data(session_analysis)
                print("ğŸ’¾ Zapisano dane uczenia")
            
            # WYCZYÅšÄ† PAMIÄ˜Ä† ASYSTENTA PRZED ROZPOCZÄ˜CIEM
            self.clean_assistant_memory()
            
            # Wybierz istotne dokumenty (maksymalnie 5)
            relevant_docs = self.select_relevant_documents(query, max_docs=5)
            print(f"ğŸ” Wybrano {len(relevant_docs)} dokumentÃ³w: {relevant_docs[:3]}...")
            
            # Ustaw liczbÄ™ uÅ¼ytych dokumentÃ³w
            self.last_documents_used = len(relevant_docs)
            
            # UtwÃ³rz vector store z dokumentami
            vector_store_id, file_ids = self.create_vector_store_with_files(relevant_docs)
            
            if not vector_store_id:
                print("âš ï¸  Nie udaÅ‚o siÄ™ utworzyÄ‡ vector store, kontynuujÄ™ bez plikÃ³w")
                
            print(f"ğŸ” Vector store ID: {vector_store_id}, Pliki: {len(file_ids)}")
            
            # Przygotuj kontekst rozmowy (peÅ‚na historia, zwiÄ™ksz do 30 wiadomoÅ›ci)
            messages = []
            # ZwiÄ™ksz kontekst do 30 ostatnich wiadomoÅ›ci (15 par pytanie-odpowiedÅº)
            recent_context = context[-30:] if len(context) > 30 else context
            
            print(f"ğŸ” PrzygotowujÄ™ kontekst z {len(recent_context)} wiadomoÅ›ci (z {len(context)} caÅ‚kowitych)")
            print(f"ğŸ“š Pierwsze pytanie w sesji: {context[0]['content'][:100] if context else 'Brak kontekstu'}...")
            print(f"ğŸ“š Ostatnie pytanie w sesji: {context[-1]['content'][:100] if context else 'Brak kontekstu'}...")
            
            # WyÅ›wietl streszczenie caÅ‚ej historii
            if len(context) > 0:
                user_questions = [msg['content'][:50] + "..." for msg in context if msg['role'] == 'user']
                print(f"ğŸ“‹ Wszystkie pytania uÅ¼ytkownika w sesji ({len(user_questions)}):")
                for i, q in enumerate(user_questions, 1):
                    print(f"   {i}. {q}")
            
            for msg in recent_context:
                messages.append({
                    "role": msg["role"],
                    "content": msg["content"]
                })
                
            # Dodaj szczegÃ³Å‚owe instrukcje dotyczÄ…ce kontekstu sesji
            context_instruction = ""
            if len(context) > 1:  # JeÅ›li jest historia rozmowy
                first_question = context[0]['content'] if context[0]['role'] == 'user' else "Brak pierwszego pytania"
                context_instruction = f"""
                
                WAÅ»NE INSTRUKCJE DOTYCZÄ„CE KONTEKSTU SESJI:
                - PamiÄ™taj, Å¼e to kontynuacja rozmowy - przeanalizuj caÅ‚Ä… historiÄ™ powyÅ¼ej
                - Pierwsze pytanie uÅ¼ytkownika w tej sesji to: "{first_question}"
                - JeÅ›li uÅ¼ytkownik pyta o coÅ›, co byÅ‚o wczeÅ›niej omawiane, odwoÅ‚aj siÄ™ do tego
                - JeÅ›li uÅ¼ytkownik pyta o pierwsze pytanie, odpowiedz konkretnie
                - Zachowaj spÃ³jnoÅ›Ä‡ ze stylem odpowiedzi preferowanym przez uÅ¼ytkownika
                - NawiÄ…zuj do wczeÅ›niejszych tematÃ³w gdy to wÅ‚aÅ›ciwe
                
                AKTUALNE PYTANIE: {query}
                """
            else:
                context_instruction = f"PYTANIE: {query}"
            
            # Kombinuj prompt uczenia z instrukcjami kontekstu
            final_query = context_instruction
            if learning_prompt:
                final_query = f"{learning_prompt}\n\n{context_instruction}"
            
            # Dodaj aktualne pytanie z promptem uczenia
            messages.append({
                "role": "user",
                "content": final_query
            })
            
            print(f"ğŸ” Przygotowano {len(messages)} wiadomoÅ›ci w kontekÅ›cie (z promptem uczenia)")
            
            # UtwÃ³rz wÄ…tek
            print(f"ğŸ” TworzÄ™ wÄ…tek z asystentem {self.assistant_id}")
            
            thread_data = {
                "messages": messages
            }
            
            # Dodaj vector store tylko jeÅ›li istnieje
            if vector_store_id:
                thread_data["tool_resources"] = {
                    "file_search": {
                        "vector_store_ids": [vector_store_id]
                    }
                }
            
            thread = self.client.beta.threads.create(**thread_data)
            
            print(f"ğŸ” WÄ…tek utworzony: {thread.id}")
            
            # Uruchom asystenta z dodatkowym zabezpieczeniem i retry
            print(f"ğŸ” Uruchamiam asystenta w trybie strumieniowym...")
            
            max_retries = 3
            retry_count = 0
            
            while retry_count < max_retries:
                try:
                    # SprawdÅº czy wÄ…tek ma aktywne runy i je anuluj
                    self.cancel_active_runs(thread.id)
                    
                    # Przygotuj parametry dla run
                    run_params = {
                        'thread_id': thread.id,
                        'assistant_id': self.assistant_id,
                        'stream': True,
                        'max_completion_tokens': 4000  # Ograniczenie dÅ‚ugoÅ›ci odpowiedzi
                    }
                    
                    # Dodaj temperature tylko jeÅ›li model go obsÅ‚uguje
                    # NiektÃ³re modele (np. o1-preview, o3-mini) nie obsÅ‚ugujÄ… temperature
                    try:
                        run_params['temperature'] = 0.7
                        run = self.client.beta.threads.runs.create(**run_params)
                    except Exception as temp_error:
                        print(f"âš ï¸  Model nie obsÅ‚uguje temperature, uÅ¼ywam bez tego parametru: {temp_error}")
                        # UsuÅ„ temperature i sprÃ³buj ponownie
                        run_params.pop('temperature', None)
                        run = self.client.beta.threads.runs.create(**run_params)
                    
                    # PrzetwÃ³rz strumieÅ„ odpowiedzi
                    response_text = ""
                    chunk_count = 0
                    stream_failed = False
                
                    for event in run:
                        if event.event == 'thread.message.delta':
                            if hasattr(event.data, 'delta') and hasattr(event.data.delta, 'content'):
                                for content in event.data.delta.content:
                                    if content.type == 'text' and hasattr(content.text, 'value'):
                                        chunk = content.text.value
                                        response_text += chunk
                                        chunk_count += 1
                                        if chunk_count <= 3:  # Loguj tylko pierwsze 3 chunki
                                            print(f"ğŸ” Otrzymano chunk #{chunk_count}: {chunk[:30]}...")
                                        yield chunk
                        elif event.event == 'thread.run.completed':
                            print("âœ… UkoÅ„czono generowanie odpowiedzi")
                            break
                        elif event.event == 'thread.run.failed':
                            error_details = getattr(event.data, 'last_error', None)
                            if error_details:
                                error_msg = f"OpenAI API Error: {error_details.code} - {error_details.message}"
                                print(f"âŒ {error_msg}")
                                stream_failed = True
                                if retry_count < max_retries - 1:
                                    print(f"ğŸ”„ PrÃ³bujÄ™ ponownie ({retry_count + 1}/{max_retries})...")
                                    break
                                else:
                                    yield f"Przepraszam, wystÄ…piÅ‚ bÅ‚Ä…d po stronie OpenAI: {error_details.message}. SprÃ³buj ponownie za chwilÄ™."
                            else:
                                print(f"âŒ BÅ‚Ä…d podczas generowania: {event.data}")
                                stream_failed = True
                                if retry_count < max_retries - 1:
                                    print(f"ğŸ”„ PrÃ³bujÄ™ ponownie ({retry_count + 1}/{max_retries})...")
                                    break
                                else:
                                    yield "Przepraszam, wystÄ…piÅ‚ nieoczekiwany bÅ‚Ä…d. SprÃ³buj ponownie."
                            break
                        elif event.event == 'thread.run.cancelled':
                            print("âš ï¸ Generowanie zostaÅ‚o anulowane")
                            yield "Generowanie odpowiedzi zostaÅ‚o anulowane."
                            return
                    
                    # JeÅ›li nie byÅ‚o bÅ‚Ä™du, zakoÅ„cz retry loop
                    if not stream_failed:
                        print(f"ğŸ” Otrzymano Å‚Ä…cznie {chunk_count} chunkÃ³w, dÅ‚ugoÅ›Ä‡ odpowiedzi: {len(response_text)}")
                        break
                        
                except Exception as stream_error:
                    print(f"âŒ BÅ‚Ä…d podczas streamowania: {stream_error}")
                    if retry_count < max_retries - 1:
                        print(f"ğŸ”„ PrÃ³bujÄ™ ponownie ({retry_count + 1}/{max_retries})...")
                        retry_count += 1
                        time.sleep(2 ** retry_count)  # Exponential backoff
                        continue
                    else:
                        yield f"Przepraszam, wystÄ…piÅ‚ bÅ‚Ä…d podczas generowania odpowiedzi: {str(stream_error)}"
                        break
                
                retry_count += 1
                if stream_failed and retry_count < max_retries:
                    time.sleep(2 ** retry_count)  # Exponential backoff
            
            # Poczekaj na zakoÅ„czenie
            time.sleep(1)
            
            # UsuÅ„ tymczasowe zasoby
            self.cleanup_resources(vector_store_id, file_ids, thread.id)
            print(f"ğŸ” ZakoÅ„czono generowanie odpowiedzi")
            
        except Exception as e:
            print(f"âŒ BÅ‚Ä…d podczas generowania odpowiedzi: {str(e)}")
            import traceback
            traceback.print_exc()
            yield f"Przepraszam, wystÄ…piÅ‚ bÅ‚Ä…d: {str(e)}"
    
    def cleanup_resources(self, vector_store_id, file_ids, thread_id):
        """Usuwa tymczasowe zasoby"""
        try:
            # UsuÅ„ pliki
            for file_id in file_ids:
                try:
                    self.client.files.delete(file_id)
                except:
                    pass
            
            # UsuÅ„ vector store
            if vector_store_id:
                try:
                    self.client.beta.vector_stores.delete(vector_store_id)
                except:
                    pass
            
            # UsuÅ„ wÄ…tek
            if thread_id:
                try:
                    self.client.beta.threads.delete(thread_id)
                except:
                    pass
                    
        except Exception as e:
            print(f"BÅ‚Ä…d podczas usuwania zasobÃ³w: {str(e)}")
    
    def generate_pdf_report(self, content, session_id, message_id=None):
        """Generuje raport PDF z odpowiedzi"""
        try:
            # UtwÃ³rz katalog dla sesji
            reports_dir = f'reports/{session_id}'
            os.makedirs(reports_dir, exist_ok=True)
            
            # Nazwa pliku PDF
            if message_id:
                filename = f'answer_{message_id}.pdf'
            else:
                filename = f'answer_{int(time.time())}.pdf'
            
            filepath = os.path.join(reports_dir, filename)
            
            # UtwÃ³rz dokument PDF
            doc = SimpleDocTemplate(filepath, pagesize=A4)
            
            # Style
            styles = getSampleStyleSheet()
            title_style = ParagraphStyle(
                'CustomTitle',
                parent=styles['Heading1'],
                fontSize=18,
                alignment=TA_CENTER,
                spaceAfter=30
            )
            
            normal_style = ParagraphStyle(
                'CustomNormal',
                parent=styles['Normal'],
                fontSize=12,
                alignment=TA_JUSTIFY,
                spaceAfter=12
            )
            
            # TreÅ›Ä‡ dokumentu
            story = []
            
            # TytuÅ‚
            story.append(Paragraph("Aero-Chat - Raport Odpowiedzi", title_style))
            story.append(Spacer(1, 12))
            
            # Data
            story.append(Paragraph(f"Data: {datetime.now().strftime('%d.%m.%Y %H:%M')}", normal_style))
            story.append(Paragraph(f"Sesja: {session_id}", normal_style))
            story.append(Spacer(1, 20))
            
            # OdpowiedÅº
            story.append(Paragraph("OdpowiedÅº:", styles['Heading2']))
            
            # Podziel treÅ›Ä‡ na akapity
            paragraphs = content.split('\n\n')
            for para in paragraphs:
                if para.strip():
                    story.append(Paragraph(para.strip(), normal_style))
            
            # Zbuduj PDF
            doc.build(story)
            
            return filepath
            
        except Exception as e:
            print(f"BÅ‚Ä…d podczas generowania PDF: {str(e)}")
            return None
    
    def add_feedback_to_training(self, feedback_data):
        """Dodaje feedback do bazy wiedzy asystenta"""
        try:
            # Zapisz feedback w specjalnym katalogu treningowym
            training_dir = 'training_data'
            os.makedirs(training_dir, exist_ok=True)
            
            # StwÃ³rz nazwÄ™ pliku bazujÄ…c na typie sekcji i timestampie
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'feedback_{feedback_data["section_type"]}_{timestamp}.json'
            filepath = os.path.join(training_dir, filename)
            
            # Przygotuj dane treningowe
            training_data = {
                'feedback_type': feedback_data['feedback_type'],
                'section_type': feedback_data['section_type'],
                'content': feedback_data['content'],
                'user_description': feedback_data['description'],
                'timestamp': feedback_data['timestamp'],
                'session_id': feedback_data['session_id'],
                'improvement_notes': self.generate_improvement_notes(feedback_data)
            }
            
            # Zapisz do pliku JSON
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(training_data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… Feedback dodany do bazy treningowej: {filename}")
            
            # JeÅ›li feedback jest negatywny, sprÃ³buj poprawiÄ‡ odpowiedÅº
            if feedback_data['feedback_type'] == 'negative':
                self.process_negative_feedback(feedback_data)
                
        except Exception as e:
            print(f"âŒ BÅ‚Ä…d podczas dodawania feedback do treningu: {str(e)}")
    
    def generate_improvement_notes(self, feedback_data):
        """Generuje notatki o poprawach na podstawie feedbacku"""
        notes = []
        
        if feedback_data['feedback_type'] == 'negative':
            notes.append("TreÅ›Ä‡ wymaga poprawy wedÅ‚ug uÅ¼ytkownika")
            if feedback_data['description']:
                notes.append(f"Uwagi uÅ¼ytkownika: {feedback_data['description']}")
                
            # Dodaj sugestie poprawy na podstawie typu sekcji
            if feedback_data['section_type'] == 'header':
                notes.append("RozwaÅ¼ zmianÄ™ struktury nagÅ‚Ã³wka lub jego treÅ›ci")
            elif feedback_data['section_type'] == 'paragraph':
                notes.append("RozwaÅ¼ przepisanie akapitu dla lepszej jasnoÅ›ci")
            elif feedback_data['section_type'] == 'list':
                notes.append("RozwaÅ¼ zmianÄ™ struktury listy lub jej elementÃ³w")
        else:
            notes.append("TreÅ›Ä‡ uznana za przydatnÄ… przez uÅ¼ytkownika")
            if feedback_data['description']:
                notes.append(f"Pozytywne uwagi: {feedback_data['description']}")
        
        return notes
    
    def process_negative_feedback(self, feedback_data):
        """Przetwarza negatywny feedback i prÃ³buje poprawiÄ‡ przyszÅ‚e odpowiedzi"""
        try:
            # Zapisz do pliku z negatywnymi feedbackami
            negative_feedback_file = 'training_data/negative_feedback_summary.json'
            
            # Wczytaj istniejÄ…ce negatywne feedbacki
            if os.path.exists(negative_feedback_file):
                with open(negative_feedback_file, 'r', encoding='utf-8') as f:
                    all_negative = json.load(f)
            else:
                all_negative = []
            
            # Dodaj nowy feedback
            all_negative.append({
                'content': feedback_data['content'],
                'description': feedback_data['description'],
                'section_type': feedback_data['section_type'],
                'timestamp': feedback_data['timestamp']
            })
            
            # Zapisz z powrotem
            with open(negative_feedback_file, 'w', encoding='utf-8') as f:
                json.dump(all_negative, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… Negatywny feedback dodany do analizy: {feedback_data['section_type']}")
            
        except Exception as e:
            print(f"âŒ BÅ‚Ä…d podczas przetwarzania negatywnego feedbacku: {str(e)}")

    def get_training_insights(self):
        """Pobiera insights z feedbacku treningowego"""
        try:
            training_dir = 'training_data'
            if not os.path.exists(training_dir):
                return {}
            
            insights = {
                'total_feedback': 0,
                'positive_feedback': 0,
                'negative_feedback': 0,
                'section_types': {},
                'common_issues': []
            }
            
            # Przeanalizuj wszystkie pliki feedbacku
            for filename in os.listdir(training_dir):
                if filename.startswith('feedback_') and filename.endswith('.json'):
                    filepath = os.path.join(training_dir, filename)
                    with open(filepath, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        
                        insights['total_feedback'] += 1
                        
                        if data['feedback_type'] == 'positive':
                            insights['positive_feedback'] += 1
                        else:
                            insights['negative_feedback'] += 1
                        
                        # Statystyki typÃ³w sekcji
                        section_type = data['section_type']
                        if section_type not in insights['section_types']:
                            insights['section_types'][section_type] = {'positive': 0, 'negative': 0}
                        insights['section_types'][section_type][data['feedback_type']] += 1
            
            return insights
            
        except Exception as e:
            print(f"âŒ BÅ‚Ä…d podczas pobierania insights: {str(e)}")
            return {}
    
    def save_conversation_context(self, session_id, context, current_message):
        """Zapisuje kontekst rozmowy do pliku dla debugowania i uczenia siÄ™"""
        try:
            os.makedirs('history', exist_ok=True)
            context_file = f'history/{session_id}_full_context.json'
            
            # Przygotuj strukturÄ™ do zapisu
            conversation_data = {
                'timestamp': datetime.now().isoformat(),
                'session_id': session_id,
                'total_messages': len(context),
                'current_message': current_message,
                'full_conversation': context,
                'summary': {
                    'user_messages': len([m for m in context if m['role'] == 'user']),
                    'assistant_messages': len([m for m in context if m['role'] == 'assistant']),
                    'last_3_messages': context[-3:] if len(context) >= 3 else context
                }
            }
            
            with open(context_file, 'w', encoding='utf-8') as f:
                json.dump(conversation_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ Kontekst rozmowy zapisany do {context_file}")
            print(f"ğŸ“Š Statystyki: {conversation_data['summary']}")
            
        except Exception as e:
            print(f"âš ï¸  BÅ‚Ä…d zapisu kontekstu rozmowy: {e}")

    def load_conversation_context(self, session_id):
        """Åaduje kontekst rozmowy z pliku"""
        try:
            context_file = f'history/{session_id}_full_context.json'
            if os.path.exists(context_file):
                with open(context_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                return data.get('full_conversation', [])
            return []
        except Exception as e:
            print(f"âš ï¸  BÅ‚Ä…d Å‚adowania kontekstu rozmowy: {e}")
            return []
